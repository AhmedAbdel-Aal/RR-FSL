{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T18:43:30.927083Z",
     "iopub.status.busy": "2021-07-12T18:43:30.926785Z",
     "iopub.status.idle": "2021-07-12T18:43:32.830009Z",
     "shell.execute_reply": "2021-07-12T18:43:32.829176Z",
     "shell.execute_reply.started": "2021-07-12T18:43:30.927052Z"
    }
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LSTM_Emitter, CRF, Hier_LSTM_CRF_Classifier\n",
    "from training import batchify, train_step, val_step, statistics, learn\n",
    "from data import prepare_data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T18:44:39.005755Z",
     "iopub.status.busy": "2021-07-12T18:44:39.005431Z",
     "iopub.status.idle": "2021-07-12T18:44:39.010373Z",
     "shell.execute_reply": "2021-07-12T18:44:39.009314Z",
     "shell.execute_reply.started": "2021-07-12T18:44:39.005725Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pretrained = True\n",
    "    data_path = '../../data/bert_sentence_independent_embeddings' ## Input to the pre-trained embedding(should contain 4 sub-folders, IT test and train, CL test and train)\n",
    "    save_path = './save/' ## path to save the model\n",
    "    device = 'cpu' ## device to be used\n",
    "    batch_size = 40 ## batch size\n",
    "    print_every = 1 ## print loss after these many epochs\n",
    "    lr = 0.01 ## learning rate\n",
    "    reg = 0 ## weight decay for Adam Opt\n",
    "    emb_dim = 768 ## the pre-trained embedding dimension of the sentences\n",
    "    hidden_dim = 384\n",
    "    epochs = 300 ## Something between 250-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T18:44:40.492545Z",
     "iopub.status.busy": "2021-07-12T18:44:40.492221Z",
     "iopub.status.idle": "2021-07-12T18:44:40.496660Z",
     "shell.execute_reply": "2021-07-12T18:44:40.495786Z",
     "shell.execute_reply.started": "2021-07-12T18:44:40.492513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./save/: File exists\r\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "#np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
    "\n",
    "## creating a directory to save models and other utility files\n",
    "!mkdir './save/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model on IT cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def load_documents(file_path):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            data_dict = pickle.load(file)\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "## path for the training and testing files\n",
    "main_input_path = '../../data/bert_sentence_independent_embeddings'\n",
    "\n",
    "path_cl_train = Path(main_input_path,'CL_train.pkl')\n",
    "path_cl_dev = Path(main_input_path,'CL_dev.pkl')\n",
    "path_cl_test = Path(main_input_path,'CL_test.pkl')\n",
    "\n",
    "path_it_train = Path(main_input_path,'IT_train.pkl')\n",
    "path_it_dev = Path(main_input_path,'IT_dev.pkl')\n",
    "path_it_test = Path(main_input_path,'IT_test.pkl')\n",
    "\n",
    "train_data_dict = load_documents(path_cl_train)\n",
    "dev_data_dict = load_documents(path_cl_train)\n",
    "test_data_dict = load_documents(path_cl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T18:47:04.240977Z",
     "iopub.status.busy": "2021-07-12T18:47:04.240638Z",
     "iopub.status.idle": "2021-07-12T18:53:12.364648Z",
     "shell.execute_reply": "2021-07-12T18:53:12.363872Z",
     "shell.execute_reply.started": "2021-07-12T18:47:04.240946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data ... Done\n"
     ]
    }
   ],
   "source": [
    "## Preparing data and Training Model for IT cases, similarly can be run for IT+CL and CL\n",
    "\n",
    "print('\\nPreparing data ...', end = ' ')\n",
    "\n",
    "x_it_train, y_it_train, x_it_dev, y_it_dev, x_it_test, y_it_test, word2idx_it, tag2idx_it = prepare_data_new(train_data_dict, dev_data_dict, test_data_dict, args, args.emb_dim)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_it_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use loaded tag2idx\n",
    "def load_config(path):\n",
    "    # Load and parse the config file\n",
    "    with open(path, 'r') as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "#tag2idx_it = load_config('./save/tag2idx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Tags IT: 14\n",
      "Dump word2idx and tag2idx\n"
     ]
    }
   ],
   "source": [
    "print('#Tags IT:', len(tag2idx_it))\n",
    "print('Dump word2idx and tag2idx')\n",
    "#with open(args.save_path + 'word2idx.json', 'w') as fp:\n",
    "#    json.dump(word2idx_it, fp)\n",
    "#with open(args.save_path + 'tag2idx.json', 'w') as fp:\n",
    "#    json.dump(tag2idx_it, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function data.prepare_data_new.<locals>.<lambda>()>,\n",
       "            {'<pad>': 0,\n",
       "             '<start>': 1,\n",
       "             '<end>': 2,\n",
       "             'Fact': 3,\n",
       "             'ArgumentPetitioner': 4,\n",
       "             'ArgumentRespondent': 5,\n",
       "             'RatioOfTheDecision': 6,\n",
       "             'RulingByPresentCourt': 7,\n",
       "             'RulingByLowerCourt': 8,\n",
       "             'PrecedentNotReliedUpon': 9,\n",
       "             'PrecedentReliedUpon': 10,\n",
       "             'Statute': 11,\n",
       "             'Issue': 12,\n",
       "             'Dissent': 13})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model for IT ... Done\n"
     ]
    }
   ],
   "source": [
    "print('\\nInitializing model for IT ...', end = ' ')   \n",
    "model = Hier_LSTM_CRF_Classifier(len(tag2idx_it), args.emb_dim, args.hidden_dim, tag2idx_it['<start>'], tag2idx_it['<end>'], tag2idx_it['<pad>'],args.device).to(args.device)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test...\n",
      "  EPOCH     Tr_LOSS   Tr_F1    Val_LOSS  Val_F1\n",
      "-----------------------------------------------------------\n",
      "      1   26445.748   0.014   27805.688   0.039\n",
      "      2   28224.742   0.063   24352.318   0.021\n",
      "      3   24833.088   0.060   20475.387   0.040\n",
      "      4   21019.412   0.069   20741.848   0.016\n",
      "      5   21271.373   0.047   21272.195   0.040\n",
      "      6   21770.471   0.074   20584.617   0.040\n",
      "      7   21057.045   0.064   20119.805   0.040\n",
      "      8   20476.148   0.067   19633.656   0.040\n",
      "      9   20015.498   0.065   19159.668   0.040\n",
      "     10   19536.176   0.061   19177.258   0.040\n",
      "     11   19615.777   0.070   19255.842   0.022\n",
      "     12   19663.312   0.066   18923.785   0.040\n",
      "     13   19322.957   0.057   18394.773   0.040\n",
      "     14   18741.980   0.057   18114.574   0.040\n",
      "     15   18472.547   0.060   18113.594   0.040\n",
      "     16   18433.490   0.056   18080.799   0.039\n",
      "     17   18333.457   0.043   17839.133   0.040\n",
      "     18   18130.461   0.052   17518.986   0.075\n",
      "     19   17740.826   0.093   17202.113   0.089\n",
      "     20   17563.082   0.093   17028.127   0.132\n",
      "     21   17247.621   0.126   16865.463   0.132\n",
      "     22   17039.312   0.133   16717.748   0.126\n",
      "     23   16972.082   0.125   16498.129   0.140\n",
      "     24   16764.449   0.126   16403.328   0.113\n",
      "     25   16666.779   0.120   16229.294   0.132\n",
      "     26   16328.648   0.130   15927.823   0.138\n",
      "     27   16183.517   0.128   15824.240   0.130\n",
      "     28   15935.518   0.137   15732.559   0.131\n",
      "     29   15821.983   0.132   15520.251   0.135\n",
      "     30   15774.164   0.137   15301.258   0.137\n",
      "     31   15648.865   0.128   15460.770   0.127\n",
      "     32   15516.395   0.124   15015.878   0.138\n",
      "     33   15436.660   0.129   15345.477   0.116\n",
      "     34   15061.877   0.132   14742.824   0.138\n",
      "     35   15027.817   0.120   14699.377   0.134\n",
      "     36   14841.473   0.139   14367.585   0.143\n",
      "     37   14652.322   0.129   14333.721   0.140\n",
      "     38   14684.936   0.135   14188.263   0.139\n",
      "     39   14393.529   0.135   14297.844   0.135\n",
      "     40   14519.146   0.129   14183.177   0.132\n",
      "     41   14119.346   0.136   13834.808   0.131\n",
      "     42   14077.872   0.132   13612.141   0.140\n",
      "     43   13957.312   0.124   13779.661   0.113\n",
      "     44   13756.326   0.133   13679.760   0.130\n",
      "     45   13647.727   0.130   13391.943   0.127\n",
      "     46   13701.579   0.128   13127.366   0.138\n",
      "     47   13435.348   0.128   13027.767   0.134\n",
      "     48   13248.100   0.127   12984.213   0.128\n",
      "     49   13059.244   0.131   12889.035   0.125\n",
      "     50   13400.346   0.124   13010.883   0.132\n",
      "     51   13149.218   0.126   12677.805   0.125\n",
      "     52   12785.156   0.130   12669.977   0.125\n",
      "     53   12744.033   0.122   12358.282   0.139\n",
      "     54   12479.321   0.126   12342.947   0.119\n",
      "     55   12742.239   0.127   12609.884   0.123\n",
      "     56   12391.003   0.127   12211.265   0.120\n",
      "     57   12242.662   0.129   12045.423   0.127\n",
      "     58   12181.504   0.122   11897.912   0.123\n",
      "     59   12115.208   0.122   11842.868   0.129\n",
      "     60   11955.124   0.134   12006.235   0.129\n",
      "     61   11874.975   0.124   11583.012   0.120\n",
      "     62   11967.896   0.124   11468.803   0.116\n",
      "     63   11934.609   0.124   11300.941   0.125\n",
      "     64   11753.307   0.131   11255.973   0.125\n",
      "     65   11474.571   0.119   11387.877   0.125\n",
      "     66   11299.671   0.129   11099.510   0.119\n",
      "     67   11375.499   0.136   10958.056   0.124\n",
      "     68   11045.653   0.124   10840.775   0.124\n",
      "     69   11112.498   0.140   10962.953   0.127\n",
      "     70   10927.819   0.121   10678.830   0.127\n",
      "     71   10889.815   0.127   10629.606   0.114\n",
      "     72   10778.601   0.119   10569.730   0.120\n",
      "     73   10643.962   0.125   10355.548   0.112\n",
      "     74   10601.385   0.122   10226.797   0.118\n",
      "     75   10537.882   0.134   10204.290   0.132\n",
      "     76   10725.153   0.130   10046.157   0.120\n",
      "     77   10300.531   0.121   10075.190   0.149\n",
      "     78   10426.781   0.133    9938.052   0.150\n",
      "     79   10099.120   0.146   10067.293   0.128\n",
      "     80   10188.275   0.132   10062.741   0.117\n",
      "     81    9915.474   0.129    9932.695   0.120\n",
      "     82   10098.013   0.131    9683.777   0.117\n",
      "     83    9917.804   0.120    9708.065   0.120\n",
      "     84    9679.351   0.128    9543.943   0.158\n",
      "     85    9814.424   0.116    9439.664   0.135\n",
      "     86    9521.943   0.152    9366.413   0.115\n",
      "     87    9463.856   0.140    9193.985   0.156\n",
      "     88    9326.498   0.152    9161.872   0.122\n",
      "     89    9163.803   0.131    9040.094   0.127\n",
      "     90    9076.215   0.154    9190.980   0.118\n",
      "     91    9115.970   0.161    8997.273   0.122\n",
      "     92    9209.471   0.129    8940.098   0.118\n",
      "     93    8824.060   0.173    8790.519   0.142\n",
      "     94    8963.310   0.140    8830.226   0.121\n",
      "     95    8945.604   0.135    8676.628   0.137\n",
      "     96    8725.325   0.145    8576.012   0.156\n",
      "     97    8802.435   0.139    8380.757   0.160\n",
      "     98    8627.520   0.134    8331.121   0.160\n",
      "     99    8578.997   0.144    8273.455   0.138\n",
      "    100    8452.548   0.137    8385.304   0.171\n",
      "    101    8319.934   0.186    8178.113   0.150\n",
      "    102    8358.006   0.156    8022.688   0.169\n",
      "    103    8266.693   0.137    7960.112   0.170\n",
      "    104    8112.588   0.176    7837.178   0.197\n",
      "    105    7985.628   0.176    7910.065   0.179\n",
      "    106    8161.448   0.161    7948.864   0.167\n",
      "    107    7794.670   0.210    7651.356   0.194\n",
      "    108    7664.375   0.197    7712.400   0.130\n",
      "    109    7681.200   0.197    7442.206   0.176\n",
      "    110    7758.153   0.188    7327.122   0.183\n",
      "    111    7556.457   0.202    7355.322   0.191\n",
      "    112    7504.678   0.195    7390.836   0.166\n",
      "    113    7536.307   0.178    7458.589   0.129\n",
      "    114    7860.395   0.155    7585.052   0.116\n",
      "    115    7807.723   0.119    7358.313   0.170\n",
      "    116    7568.526   0.164    7504.534   0.107\n",
      "    117    7758.145   0.147    7245.800   0.163\n",
      "    118    7581.860   0.155    7361.318   0.146\n",
      "    119    7337.483   0.188    7429.428   0.121\n",
      "    120    7629.711   0.117    7107.171   0.120\n",
      "    121    7314.681   0.135    6954.348   0.202\n",
      "    122    7139.558   0.188    6914.165   0.215\n",
      "    123    7202.014   0.198    6896.175   0.186\n",
      "    124    7011.996   0.174    6847.333   0.144\n",
      "    125    6972.327   0.163    6825.583   0.142\n",
      "    126    7075.703   0.166    6670.454   0.213\n",
      "    127    6723.289   0.227    6644.640   0.212\n",
      "    128    6915.666   0.208    6798.912   0.133\n",
      "    129    6791.893   0.139    6670.745   0.144\n",
      "    130    6781.418   0.171    6454.402   0.181\n",
      "    131    6701.549   0.188    6652.698   0.156\n",
      "    132    6583.192   0.186    6323.132   0.195\n",
      "    133    6683.707   0.196    6615.072   0.145\n",
      "    134    6443.870   0.173    6262.063   0.178\n",
      "    135    6463.730   0.171    6254.701   0.176\n",
      "    136    6458.634   0.186    6249.783   0.193\n",
      "    137    6894.773   0.157    6254.998   0.203\n",
      "    138    6793.085   0.167    6465.674   0.164\n",
      "    139    6244.122   0.231    6105.833   0.151\n",
      "    140    6654.767   0.133    6089.087   0.165\n",
      "    141    6587.854   0.140    6246.830   0.154\n",
      "    142    6376.163   0.173    6061.211   0.190\n",
      "    143    6348.785   0.189    6024.190   0.167\n",
      "    144    6115.903   0.200    6018.120   0.147\n",
      "    145    6140.571   0.153    6106.628   0.143\n",
      "    146    6341.079   0.140    5878.090   0.162\n",
      "    147    6018.058   0.187    5866.438   0.184\n",
      "    148    6010.392   0.189    5831.230   0.195\n",
      "    149    6115.697   0.186    5917.892   0.152\n",
      "    150    6124.865   0.141    5953.976   0.151\n",
      "    151    6071.302   0.174    5911.438   0.202\n",
      "    152    6081.532   0.211    6115.461   0.151\n",
      "    153    6386.668   0.138    6315.071   0.148\n",
      "    154    6596.714   0.156    6395.631   0.079\n",
      "    155    6597.406   0.067    6416.323   0.098\n",
      "    156    6573.554   0.101    6099.465   0.140\n",
      "    157    6148.061   0.145    5666.317   0.184\n",
      "    158    5938.896   0.158    5917.363   0.143\n",
      "    159    6086.003   0.140    6177.932   0.155\n",
      "    160    6499.909   0.131    6108.428   0.159\n",
      "    161    6352.103   0.150    6015.870   0.096\n",
      "    162    6262.054   0.089    6874.370   0.077\n",
      "    163    6950.480   0.085    7762.210   0.113\n",
      "    164    8060.676   0.099    7130.596   0.138\n",
      "    165    7166.988   0.168    6463.592   0.154\n",
      "    166    6697.792   0.141    6994.740   0.100\n",
      "    167    7084.019   0.083    7294.018   0.098\n",
      "    168    7309.674   0.123    7571.739   0.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    169    7589.035   0.133    7153.213   0.069\n",
      "    170    7259.565   0.087    6333.454   0.189\n",
      "    171    6301.384   0.176    6874.673   0.128\n",
      "    172    7187.184   0.100    7141.773   0.128\n",
      "    173    7149.940   0.131    6770.953   0.131\n",
      "    174    6957.550   0.143    7686.480   0.094\n",
      "    175    7614.180   0.152    8039.624   0.091\n",
      "    176    8024.770   0.119    6946.123   0.167\n",
      "    177    7644.292   0.120    7458.808   0.133\n",
      "    178    7110.850   0.155    6090.658   0.184\n",
      "    179    6396.981   0.229    6523.886   0.169\n",
      "    180    6691.164   0.163    6418.642   0.111\n",
      "    181    6328.525   0.116    6052.525   0.081\n",
      "    182    5906.412   0.096    5487.561   0.118\n",
      "    183    5660.809   0.129    5925.110   0.061\n",
      "    184    5848.146   0.114    5841.479   0.092\n",
      "    185    5875.590   0.136    5515.171   0.150\n",
      "    186    5857.179   0.153    5428.770   0.126\n",
      "    187    6119.575   0.136    5718.940   0.184\n",
      "    188    6010.889   0.167    5757.693   0.164\n",
      "    189    6275.945   0.205    5612.311   0.188\n",
      "    190    5795.164   0.187    5574.694   0.141\n",
      "    191    5681.957   0.163    5311.180   0.170\n",
      "    192    5552.666   0.218    5138.619   0.162\n",
      "    193    5544.199   0.178    5397.185   0.135\n",
      "    194    5153.713   0.174    5387.345   0.129\n",
      "    195    5422.396   0.179    5295.441   0.125\n",
      "    196    5262.999   0.160    5164.585   0.151\n",
      "    197    5294.545   0.154    4977.597   0.210\n",
      "    198    5408.028   0.173    5226.670   0.160\n",
      "    199    5254.157   0.228    4953.537   0.198\n",
      "    200    5175.970   0.167    4911.190   0.169\n",
      "    201    5071.378   0.215    4813.123   0.166\n",
      "    202    4913.171   0.172    4852.224   0.176\n",
      "    203    4950.100   0.180    4863.572   0.155\n",
      "    204    5147.220   0.154    4659.690   0.180\n",
      "    205    4792.960   0.193    4703.411   0.213\n",
      "    206    4962.090   0.165    5103.829   0.186\n",
      "    207    5146.630   0.170    5092.102   0.158\n",
      "    208    5090.192   0.159    4899.637   0.195\n",
      "    209    5011.828   0.193    4809.825   0.198\n",
      "    210    4917.172   0.208    4739.404   0.206\n",
      "    211    4888.408   0.196    4643.525   0.198\n",
      "    212    4718.487   0.183    4682.111   0.199\n",
      "    213    4530.963   0.261    4567.286   0.181\n",
      "    214    4881.709   0.205    4512.085   0.197\n",
      "    215    4546.476   0.234    4451.501   0.203\n",
      "    216    4637.461   0.214    4463.652   0.171\n",
      "    217    4602.215   0.179    4512.278   0.219\n",
      "    218    4592.110   0.215    4754.397   0.185\n",
      "    219    4538.508   0.194    4313.682   0.197\n",
      "    220    4645.372   0.188    4210.520   0.233\n",
      "    221    4463.038   0.189    4580.656   0.249\n",
      "    222    4741.288   0.268    4518.997   0.204\n",
      "    223    4682.420   0.171    4698.158   0.192\n",
      "    224    4606.024   0.189    4717.172   0.178\n",
      "    225    4777.854   0.187    4455.428   0.226\n",
      "    226    4478.369   0.210    4731.049   0.229\n",
      "    227    4466.024   0.278    4373.995   0.211\n",
      "    228    4284.216   0.231    4348.397   0.177\n",
      "    229    4603.539   0.168    4380.677   0.239\n",
      "    230    4423.262   0.215    4378.348   0.204\n",
      "    231    4421.880   0.256    4263.166   0.217\n",
      "    232    4477.970   0.183    4103.838   0.198\n",
      "    233    4410.197   0.186    4265.212   0.202\n",
      "    234    4225.517   0.268    4208.260   0.193\n",
      "    235    4254.724   0.246    4356.919   0.162\n",
      "    236    4349.747   0.154    4113.769   0.269\n",
      "    237    4243.653   0.257    4468.965   0.198\n",
      "    238    4354.715   0.230    4206.726   0.163\n",
      "    239    4550.067   0.198    4417.617   0.167\n",
      "    240    4759.474   0.134    4815.318   0.168\n",
      "    241    4767.803   0.162    4502.758   0.167\n",
      "    242    4739.379   0.171    4869.933   0.160\n",
      "    243    4904.648   0.161    5595.933   0.117\n",
      "    244    5880.050   0.103    5042.578   0.203\n",
      "    245    5347.729   0.183    5311.209   0.199\n",
      "    246    5018.581   0.240    5219.361   0.104\n",
      "    247    5050.406   0.161    5520.496   0.138\n",
      "    248    5500.534   0.128    5369.561   0.125\n",
      "    249    5503.719   0.126    5620.569   0.169\n",
      "    250    5865.823   0.156    6193.558   0.149\n",
      "    251    6220.879   0.146    5016.371   0.139\n",
      "    252    5009.583   0.142    5965.549   0.108\n",
      "    253    6180.792   0.089    7260.368   0.072\n",
      "    254    7175.810   0.081    6797.294   0.118\n",
      "    255    6922.453   0.126    6116.790   0.165\n",
      "    256    6919.902   0.169    6980.961   0.112\n",
      "    257    7132.023   0.096    6916.639   0.082\n",
      "    258    7254.764   0.086    6152.448   0.110\n",
      "    259    6180.800   0.123    5120.891   0.114\n",
      "    260    5234.326   0.105    5535.255   0.138\n",
      "    261    5819.341   0.142    6424.599   0.138\n",
      "    262    6430.925   0.104    5190.611   0.149\n",
      "    263    5359.964   0.212    5347.454   0.141\n",
      "    264    5259.453   0.195    6020.039   0.106\n",
      "    265    6316.854   0.125    5804.188   0.075\n",
      "    266    5672.750   0.098    4730.585   0.072\n",
      "    267    4935.597   0.106    5570.568   0.098\n",
      "    268    5489.715   0.106    5221.209   0.144\n",
      "    269    5483.833   0.107    5513.835   0.117\n",
      "    270    5595.794   0.127    5138.964   0.141\n",
      "    271    5323.375   0.146    4834.878   0.174\n",
      "    272    4984.559   0.161    5017.531   0.090\n",
      "    273    5153.262   0.099    5266.946   0.028\n",
      "    274    5396.265   0.056    4934.351   0.142\n",
      "    275    5015.510   0.169    5054.606   0.156\n",
      "    276    5137.982   0.197    5258.918   0.167\n",
      "    277    5388.807   0.160    5306.700   0.142\n",
      "    278    5082.254   0.189    4701.486   0.185\n",
      "    279    4957.785   0.185    4475.200   0.177\n",
      "    280    4676.404   0.175    4559.804   0.153\n",
      "    281    4818.750   0.164    5066.493   0.133\n",
      "    282    5286.249   0.128    4283.632   0.166\n",
      "    283    4699.124   0.158    4401.146   0.185\n",
      "    284    4697.190   0.146    4774.694   0.174\n",
      "    285    4939.250   0.168    5086.326   0.143\n",
      "    286    5161.936   0.162    4705.327   0.214\n",
      "    287    4958.453   0.204    4739.106   0.217\n",
      "    288    5054.725   0.205    5049.836   0.220\n",
      "    289    5280.595   0.188    5061.712   0.228\n",
      "    290    5177.000   0.210    5114.603   0.184\n",
      "    291    5326.941   0.179    5050.176   0.191\n",
      "    292    5332.934   0.188    5229.735   0.190\n",
      "    293    5142.047   0.220    4917.281   0.220\n",
      "    294    5032.392   0.233    4841.186   0.201\n",
      "    295    4949.187   0.175    4577.373   0.197\n",
      "    296    4784.636   0.174    4888.339   0.157\n",
      "    297    4679.328   0.173    4662.126   0.195\n",
      "    298    4605.043   0.194    4493.021   0.206\n",
      "    299    4538.449   0.190    4384.627   0.193\n",
      "    300    4541.083   0.182    4446.380   0.194\n",
      "Dumping model and data ... Done\n",
      "Time taken: 2507 secs\n",
      "Results on Validation data\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                  Fact      0.566     0.624     0.594      2979\n",
      "    ArgumentPetitioner      0.000     0.000     0.000       976\n",
      "    ArgumentRespondent      1.000     0.312     0.476       996\n",
      "    RatioOfTheDecision      0.402     0.841     0.544      2856\n",
      "  RulingByPresentCourt      1.000     0.248     0.397       113\n",
      "    RulingByLowerCourt      0.000     0.000     0.000       438\n",
      "PrecedentNotReliedUpon      0.000     0.000     0.000       209\n",
      "   PrecedentReliedUpon      0.000     0.000     0.000      1305\n",
      "               Statute      0.982     0.447     0.614       367\n",
      "                 Issue      0.000     0.000     0.000        17\n",
      "               Dissent      0.229     0.634     0.337       284\n",
      "\n",
      "              accuracy                          0.469     10540\n",
      "             macro avg      0.380     0.282     0.269     10540\n",
      "          weighted avg      0.415     0.469     0.395     10540\n",
      "\n",
      "Results on Test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmed/Lnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ahmed/Lnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ahmed/Lnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                  Fact      0.571     0.577     0.574      2979\n",
      "    ArgumentPetitioner      0.026     0.002     0.004       976\n",
      "    ArgumentRespondent      0.107     0.221     0.144       996\n",
      "    RatioOfTheDecision      0.386     0.521     0.443      2856\n",
      "  RulingByPresentCourt      0.946     0.310     0.467       113\n",
      "    RulingByLowerCourt      0.000     0.000     0.000       438\n",
      "PrecedentNotReliedUpon      0.000     0.000     0.000       209\n",
      "   PrecedentReliedUpon      0.315     0.321     0.318      1305\n",
      "               Statute      0.939     0.463     0.620       367\n",
      "                 Issue      0.000     0.000     0.000        17\n",
      "               Dissent      0.000     0.000     0.000       284\n",
      "\n",
      "              accuracy                          0.385     10540\n",
      "             macro avg      0.299     0.220     0.234     10540\n",
      "          weighted avg      0.360     0.385     0.362     10540\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmed/Lnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ahmed/Lnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ahmed/Lnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating on test...')        \n",
    "learn(model, x_it_train, y_it_train, x_it_dev, y_it_dev, x_it_test, y_it_test, tag2idx_it, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lnlp",
   "language": "python",
   "name": "lnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
