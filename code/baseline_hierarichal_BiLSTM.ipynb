{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962430b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from allennlp.common.util import pad_sequence_to_length\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c30f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## path for the training and testing files\n",
    "main_path = '/Users/ahmed/Desktop/SS23/practical-legalNLP/'\n",
    "BERT_VOCAB = \"bert-base-uncased\"\n",
    "MAX_SEQ_LENGTH = 128\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_VOCAB, do_lower_case=True)\n",
    "\n",
    "label_mapper = {\n",
    "    \"Fact\": \"Fact\",\n",
    "    \"Issue\": \"Fact\",\n",
    "    \"ArgumentPetitioner\": \"Argument\",\n",
    "    \"ArgumentRespondent\": \"Argument\",\n",
    "    \"PrecedentReliedUpon\": \"Precedent\",\n",
    "    \"PrecedentNotReliedUpon\": \"Precedent\",\n",
    "    \"PrecedentOverruled\": \"Precedent\",\n",
    "    \"RatioOfTheDecision\": \"Ratio\",\n",
    "    \"RulingByLowerCourt\": \"RulingL\",\n",
    "    \"RulingByPresentCourt\": \"RulingP\",\n",
    "    \"Statute\": \"Statute\",\n",
    "    \"Dissent\": \"Dissent\",\n",
    "    \"None\": \"None\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057c4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(path) -> dict:\n",
    "    with open(path,'r') as f:\n",
    "        d = json.load(f)\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def save_dict_to_json(dictionary, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(dictionary, file)\n",
    "    print(f\"Dictionary saved as JSON file: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058eef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dict_structure(input_dict, meta):\n",
    "    transformed_list = []\n",
    "    \n",
    "    for doc_id, doc in enumerate(input_dict.keys()):\n",
    "        transformed_dict = {}\n",
    "        document_name = doc\n",
    "        transformed_dict['id'] = meta + '_' + str(doc_id)\n",
    "        transformed_dict['annotations'] = []\n",
    "\n",
    "        sentences = input_dict[document_name]['sentences']\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = sentences[2:-2].split('\\', \\'')\n",
    "        labels = input_dict[document_name]['complete']\n",
    "        \n",
    "        # remove sentences with None labels\n",
    "        new_sentences = []\n",
    "        new_labels = []\n",
    "        for j,l in enumerate(labels):\n",
    "            if (l == 'None'):\n",
    "                continue\n",
    "            else:\n",
    "                new_sentences.append(sentences[j])\n",
    "                new_labels.append(labels[j])\n",
    "                        \n",
    "        start, end = 0, 0\n",
    "        transformed_dict['annotations'] = [{'result':[]}]\n",
    "        for i, sentence in enumerate(new_sentences):\n",
    "            end = start + len(sentence)\n",
    "            value = {'text': sentence, 'labels': new_labels[i], 'start':start, 'end':end}\n",
    "            start = end\n",
    "            annotation = {'id': i, 'value': value}\n",
    "            transformed_dict['annotations'][0]['result'].append(annotation)\n",
    "\n",
    "        transformed_dict['data'] = ' '.join(sentences)\n",
    "        transformed_dict['meta'] = meta\n",
    "        transformed_list.append(transformed_dict)\n",
    "    return transformed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c42536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pipeline(part):\n",
    "    # Construct Paths\n",
    "    path_train = Path(main_path, 'data','malik_data', part, f'{part}_train.json')\n",
    "    path_dev = Path(main_path, 'data','malik_data', part, f'{part}_dev.json')\n",
    "    path_test = Path(main_path, 'data','malik_data', part, f'{part}_test.json')\n",
    "\n",
    "    save_path_train = Path(main_path, 'rhetorical-role-baseline/datasets/malik', f'{part}_train.json')\n",
    "    save_path_dev = Path(main_path, 'rhetorical-role-baseline/datasets/malik', f'{part}_dev.json')\n",
    "    save_path_test = Path(main_path, 'rhetorical-role-baseline/datasets/malik', f'{part}_test.json')\n",
    "\n",
    "\n",
    "    # load json files\n",
    "    train_json = load_json_file(path_train)\n",
    "    dev_json = load_json_file(path_dev)\n",
    "    test_json = load_json_file(path_test)\n",
    "\n",
    "    # transform to new structure\n",
    "    train_json = transform_dict_structure(train_json, part)\n",
    "    dev_json = transform_dict_structure(dev_json, part)\n",
    "    test_json = transform_dict_structure(test_json, part)\n",
    "\n",
    "    # save new json files\n",
    "    save_dict_to_json(train_json, save_path_train)\n",
    "    save_dict_to_json(dev_json, save_path_dev)\n",
    "    save_dict_to_json(test_json, save_path_test)\n",
    "    \n",
    "    return train_json, dev_json, test_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e22110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved as JSON file: /Users/ahmed/Desktop/SS23/practical-legalNLP/rhetorical-role-baseline/datasets/malik/CL_train.json\n",
      "Dictionary saved as JSON file: /Users/ahmed/Desktop/SS23/practical-legalNLP/rhetorical-role-baseline/datasets/malik/CL_dev.json\n",
      "Dictionary saved as JSON file: /Users/ahmed/Desktop/SS23/practical-legalNLP/rhetorical-role-baseline/datasets/malik/CL_test.json\n"
     ]
    }
   ],
   "source": [
    "train, dev, test = transform_pipeline('CL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba82634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\n",
    "    \"Fact\": \"Fact\",\n",
    "    \"Issue\": \"Fact\",\n",
    "    \"ArgumentPetitioner\": \"Argument\",\n",
    "    \"ArgumentRespondent\": \"Argument\",\n",
    "    \"PrecedentReliedUpon\": \"Precedent\",\n",
    "    \"PrecedentNotReliedUpon\": \"Precedent\",\n",
    "    \"PrecedentOverruled\": \"Precedent\",\n",
    "    \"RatioOfTheDecision\": \"Ratio\",\n",
    "    \"RulingByLowerCourt\": \"RulingL\",\n",
    "    \"RulingByPresentCourt\": \"RulingP\",\n",
    "    \"Statute\": \"Statute\",\n",
    "    \"Dissent\": \"Fact\",\n",
    "    \"None\": \"None\"\n",
    "}\n",
    "\n",
    "        \n",
    "def write_in_hsln_json_format(input_dict, hsln_format_txt_dirpath, tokenizer):\n",
    "    final_string = ''\n",
    "    all_doc_toknized={}\n",
    "    for file in input_dict:\n",
    "        doc_tokenized = {}\n",
    "        labels = []\n",
    "        tokenized_sentences = []\n",
    "        attention_masks = []\n",
    "        for annotation in file['annotations'][0]['result']:\n",
    "            sentence_label = label_mapper[annotation['value']['labels']]\n",
    "            labels.append(sentence_label)\n",
    "            \n",
    "            \n",
    "            sentence_txt=annotation['value']['text']\n",
    "            sentence_txt = sentence_txt.replace(\"\\r\", \"\")\n",
    "            if sentence_txt.strip() != \"\":\n",
    "                sent_tokens = tokenizer.encode(sentence_txt, add_special_tokens=True, max_length=128)\n",
    "                tokenized_sentences.append(sent_tokens)\n",
    "            mask = [1]*(len(sent_tokens))\n",
    "            new_mask = [yi if xi !=0 else 0 for yi,xi in zip(mask,sent_tokens)]\n",
    "            attention_masks.append(new_mask)\n",
    "        \n",
    "        doc_tokenized =  {\n",
    "            \"sentence_mask\": [1] * len(tokenized_sentences),\n",
    "            \"input_ids\": tokenized_sentences,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"label_ids\": labels,\n",
    "            \"doc_name\": file['id']\n",
    "        }\n",
    "        all_doc_toknized[file['id']] = doc_tokenized\n",
    "\n",
    "\n",
    "            \n",
    "    with open(hsln_format_txt_dirpath, 'w') as fp:\n",
    "        json.dump(all_doc_toknized, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78a9573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_in_hsln_json_format(train,'datasets/malik/train_scibert.json',tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71ea6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_in_hsln_json_format(dev,'datasets/malik/dev_scibert.json',tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffa4156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_in_hsln_json_format(test,'datasets/malik/test_scibert.json',tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lnlp",
   "language": "python",
   "name": "lnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
